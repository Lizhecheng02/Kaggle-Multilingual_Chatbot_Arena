{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import secrets\n",
    "import string\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = \"\"\n",
    "\n",
    "round = 1\n",
    "\n",
    "start_idx = 0\n",
    "end_idx = 1\n",
    "\n",
    "models = [\n",
    "    \"mistralai/Mistral-Nemo-Instruct-2407\",\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-72B-Instruct\",\n",
    "    \"01-ai/Yi-1.5-34B-Chat\",\n",
    "    \"microsoft/Phi-3.5-mini-instruct\",\n",
    "    \"google/gemma-2-27b-it\",\n",
    "    \"google/gemma-2-9b-it\",\n",
    "    \"Qwen/QwQ-32B-Preview\",\n",
    "    \"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    \"meta-llama/Llama-3.1-70B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-72B-Instruct\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unique_strings(count, length):\n",
    "    unique_strings = set()\n",
    "    characters = string.ascii_letters + string.digits\n",
    "    while len(unique_strings) < count:\n",
    "        random_string = \"\".join(secrets.choice(characters) for _ in range(length))\n",
    "        unique_strings.add(random_string)\n",
    "    return list(unique_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"CohereForAI/aya_dataset\")\n",
    "df = Dataset.to_pandas(ds[\"train\"])\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"language\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vie_df = df[df[\"language\"] == \"Vietnamese\"]\n",
    "print(vie_df.shape)\n",
    "vie_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "vie_df = vie_df.sample(frac=1.0, random_state=7)\n",
    "vie_df = vie_df[start_idx:end_idx]\n",
    "vie_df.reset_index(drop=True, inplace=True)\n",
    "print(vie_df.shape)\n",
    "\n",
    "vie_df = vie_df[[\"inputs\", \"targets\", \"language\"]]\n",
    "\n",
    "ids = generate_unique_strings(len(vie_df), 25)\n",
    "prompts = []\n",
    "response_as = []\n",
    "response_bs = []\n",
    "model_as = []\n",
    "model_bs = []\n",
    "\n",
    "for idx, row in tqdm(vie_df.iterrows(), total=len(vie_df)):\n",
    "    try:\n",
    "        prompt = row[\"inputs\"]\n",
    "        response_a = row[\"targets\"]\n",
    "        model_a = \"Unknown\"\n",
    "        model_b_name = random.choice(models)\n",
    "        model_b = model_b_name.split(\"/\")[-1]\n",
    "\n",
    "        API_URL = f\"https://api-inference.huggingface.co/models/{model_b_name}\"\n",
    "        headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n",
    "        payload = {\"inputs\": prompt}\n",
    "\n",
    "        response = requests.post(API_URL, headers=headers, json=payload)\n",
    "        response_b = response.json()[0][\"generated_text\"]\n",
    "\n",
    "        prompts.append(prompt)\n",
    "        response_as.append(response_a)\n",
    "        response_bs.append(response_b)\n",
    "        model_as.append(model_a)\n",
    "        model_bs.append(model_b)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e} Row {idx}\")\n",
    "\n",
    "vie_df.drop(columns=[\"inputs\", \"targets\"], inplace=True)\n",
    "vie_df[\"id\"] = ids\n",
    "vie_df[\"prompt\"] = prompts\n",
    "vie_df[\"response_a\"] = response_as\n",
    "vie_df[\"response_b\"] = response_bs\n",
    "vie_df[\"model_a\"] = model_as\n",
    "vie_df[\"model_b\"] = model_bs\n",
    "\n",
    "vie_df = vie_df[[\"id\", \"prompt\", \"response_a\", \"response_b\", \"model_a\", \"model_b\", \"language\"]]\n",
    "vie_df.to_parquet(f\"huggingface_api_vie_{round}_{start_idx}_{end_idx}.parquet\", index=False)\n",
    "vie_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_df = df[df[\"language\"] == \"Simplified Chinese\"]\n",
    "print(zh_df.shape)\n",
    "zh_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "zh_df = zh_df.sample(frac=1.0, random_state=7)\n",
    "zh_df = zh_df[start_idx:end_idx]\n",
    "zh_df.reset_index(drop=True, inplace=True)\n",
    "print(zh_df.shape)\n",
    "\n",
    "zh_df = zh_df[[\"inputs\", \"targets\"]]\n",
    "\n",
    "ids = generate_unique_strings(len(zh_df), 26)\n",
    "prompts = []\n",
    "response_as = []\n",
    "response_bs = []\n",
    "model_as = []\n",
    "model_bs = []\n",
    "languages = []\n",
    "\n",
    "for idx, row in tqdm(zh_df.iterrows(), total=len(zh_df)):\n",
    "    try:\n",
    "        prompt = row[\"inputs\"]\n",
    "        response_a = row[\"targets\"]\n",
    "        model_a = \"Unknown\"\n",
    "        model_b_name = random.choice(models)\n",
    "        model_b = model_b_name.split(\"/\")[-1]\n",
    "\n",
    "        API_URL = f\"https://api-inference.huggingface.co/models/{model_b_name}\"\n",
    "        headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n",
    "        payload = {\"inputs\": prompt}\n",
    "\n",
    "        response = requests.post(API_URL, headers=headers, json=payload)\n",
    "        response_b = response.json()[0][\"generated_text\"]\n",
    "\n",
    "        prompts.append(prompt)\n",
    "        response_as.append(response_a)\n",
    "        response_bs.append(response_b)\n",
    "        model_as.append(model_a)\n",
    "        model_bs.append(model_b)\n",
    "        languages.append(\"Chinese\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e} Row {idx}\")\n",
    "\n",
    "zh_df.drop(columns=[\"inputs\", \"targets\"], inplace=True)\n",
    "zh_df[\"id\"] = ids\n",
    "zh_df[\"prompt\"] = prompts\n",
    "zh_df[\"response_a\"] = response_as\n",
    "zh_df[\"response_b\"] = response_bs\n",
    "zh_df[\"model_a\"] = model_as\n",
    "zh_df[\"model_b\"] = model_bs\n",
    "zh_df[\"language\"] = languages\n",
    "\n",
    "zh_df = zh_df[[\"id\", \"prompt\", \"response_a\", \"response_b\", \"model_a\", \"model_b\", \"language\"]]\n",
    "zh_df.to_parquet(f\"huggingface_api_zh_{round}_{start_idx}_{end_idx}.parquet\", index=False)\n",
    "zh_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_df = df[df[\"language\"] == \"Russian\"]\n",
    "print(ru_df.shape)\n",
    "ru_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "ru_df = ru_df.sample(frac=1.0, random_state=7)\n",
    "ru_df = ru_df[start_idx:end_idx]\n",
    "ru_df.reset_index(drop=True, inplace=True)\n",
    "print(ru_df.shape)\n",
    "\n",
    "ru_df = ru_df[[\"inputs\", \"targets\", \"language\"]]\n",
    "\n",
    "ids = generate_unique_strings(len(ru_df), 27)\n",
    "prompts = []\n",
    "response_as = []\n",
    "response_bs = []\n",
    "model_as = []\n",
    "model_bs = []\n",
    "\n",
    "for idx, row in tqdm(ru_df.iterrows(), total=len(ru_df)):\n",
    "    try:\n",
    "        prompt = row[\"inputs\"]\n",
    "        response_a = row[\"targets\"]\n",
    "        model_a = \"Unknown\"\n",
    "        model_b_name = random.choice(models)\n",
    "        model_b = model_b_name.split(\"/\")[-1]\n",
    "\n",
    "        API_URL = f\"https://api-inference.huggingface.co/models/{model_b_name}\"\n",
    "        headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n",
    "        payload = {\"inputs\": prompt}\n",
    "\n",
    "        response = requests.post(API_URL, headers=headers, json=payload)\n",
    "        response_b = response.json()[0][\"generated_text\"]\n",
    "\n",
    "        prompts.append(prompt)\n",
    "        response_as.append(response_a)\n",
    "        response_bs.append(response_b)\n",
    "        model_as.append(model_a)\n",
    "        model_bs.append(model_b)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e} Row {idx}\")\n",
    "\n",
    "ru_df.drop(columns=[\"inputs\", \"targets\"], inplace=True)\n",
    "ru_df[\"id\"] = ids\n",
    "ru_df[\"prompt\"] = prompts\n",
    "ru_df[\"response_a\"] = response_as\n",
    "ru_df[\"response_b\"] = response_bs\n",
    "ru_df[\"model_a\"] = model_as\n",
    "ru_df[\"model_b\"] = model_bs\n",
    "\n",
    "ru_df = ru_df[[\"id\", \"prompt\", \"response_a\", \"response_b\", \"model_a\", \"model_b\", \"language\"]]\n",
    "ru_df.to_parquet(f\"huggingface_api_ru_{round}_{start_idx}_{end_idx}.parquet\", index=False)\n",
    "ru_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
