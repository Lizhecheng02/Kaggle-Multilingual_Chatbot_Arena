{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import random\n",
    "import secrets\n",
    "import string\n",
    "import requests\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(os.getcwd(), \"..\", \"config.yaml\")\n",
    "\n",
    "with open(config_path, \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "openai_api_key = config[\"openai\"][\"api_key\"]\n",
    "openai_organization = config[\"openai\"][\"organization\"]\n",
    "hf_token = config[\"huggingface\"][\"token\"]\n",
    "client_openai = OpenAI(api_key=openai_api_key, organization=openai_organization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = 0\n",
    "end_idx = 3\n",
    "\n",
    "models = [\n",
    "    \"mistralai/Mistral-Nemo-Instruct-2407\",\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-72B-Instruct\",\n",
    "    \"01-ai/Yi-1.5-34B-Chat\",\n",
    "    \"microsoft/Phi-3.5-mini-instruct\",\n",
    "    \"google/gemma-2-27b-it\",\n",
    "    \"google/gemma-2-9b-it\",\n",
    "    \"Qwen/QwQ-32B-Preview\",\n",
    "    \"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    \"meta-llama/Llama-3.1-70B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-72B-Instruct\",\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-3.5-turbo-0125\",\n",
    "    \"gpt-4o\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_openai_prompt(model, prompt, temperature=0.7, top_p=0.95, max_tokens=1024):\n",
    "    completion = client_openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_completion_tokens=max_tokens\n",
    "    )\n",
    "    response = completion.choices[0].message.content.strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unique_strings(count, length):\n",
    "    unique_strings = set()\n",
    "    characters = string.ascii_letters + string.digits\n",
    "    while len(unique_strings) < count:\n",
    "        random_string = \"\".join(secrets.choice(characters) for _ in range(length))\n",
    "        unique_strings.add(random_string)\n",
    "    return list(unique_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"wenbopan/Chinese-dpo-pairs\")\n",
    "df = Dataset.to_pandas(ds[\"train\"])\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_df = df.copy()\n",
    "zh_df = zh_df.sample(frac=1.0, random_state=7)\n",
    "zh_df = zh_df[start_idx:end_idx]\n",
    "zh_df.reset_index(drop=True, inplace=True)\n",
    "print(zh_df.shape)\n",
    "\n",
    "zh_df = zh_df[[\"prompt\"]]\n",
    "\n",
    "prompts = []\n",
    "response_as = []\n",
    "response_bs = []\n",
    "model_as = []\n",
    "model_bs = []\n",
    "languages = []\n",
    "\n",
    "for idx, row in tqdm(zh_df.iterrows(), total=len(zh_df)):\n",
    "    try:\n",
    "        prompt = row[\"prompt\"]\n",
    "        print(\"Prompt:\", prompt)\n",
    "        selected_models = random.sample(models, 2)\n",
    "        print(\"Selected Models:\", selected_models)\n",
    "\n",
    "        for order, selected_model in enumerate(selected_models):\n",
    "            if \"gpt\" in selected_model and order == 0:\n",
    "                response_a = get_response_openai_prompt(model=selected_model, prompt=prompt)\n",
    "                print(\"OpenAI Response A:\", response_a)\n",
    "                model_a = selected_model\n",
    "            elif \"gpt\" in selected_model and order == 1:\n",
    "                response_b = get_response_openai_prompt(model=selected_model, prompt=prompt)\n",
    "                print(\"OpenAI Response B:\", response_b)\n",
    "                model_b = selected_model\n",
    "            elif \"gpt\" not in selected_model and order == 0:\n",
    "                API_URL = f\"https://api-inference.huggingface.co/models/{selected_model}\"\n",
    "                headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n",
    "                payload = {\"inputs\": prompt}\n",
    "                response_a = requests.post(API_URL, headers=headers, json=payload).json()[0][\"generated_text\"]\n",
    "                print(\"HF Response A:\", response_a)\n",
    "                model_a = selected_model.split(\"/\")[-1]\n",
    "            elif \"gpt\" not in selected_model and order == 1:\n",
    "                API_URL = f\"https://api-inference.huggingface.co/models/{selected_model}\"\n",
    "                headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n",
    "                payload = {\"inputs\": prompt}\n",
    "                response_b = requests.post(API_URL, headers=headers, json=payload).json()[0][\"generated_text\"]\n",
    "                print(\"HF Response B:\", response_b)\n",
    "                model_b = selected_model.split(\"/\")[-1]\n",
    "\n",
    "        languages.append(\"Chinese\")\n",
    "        prompts.append(prompt)\n",
    "        response_as.append(response_a)\n",
    "        response_bs.append(response_b)\n",
    "        model_as.append(model_a)\n",
    "        model_bs.append(model_b)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e} Row {idx}\")\n",
    "\n",
    "zh_df.drop(columns=[\"prompt\"], inplace=True)\n",
    "zh_df[\"id\"] = generate_unique_strings(count=len(prompts), length=15)\n",
    "zh_df[\"prompt\"] = prompts\n",
    "zh_df[\"response_a\"] = response_as\n",
    "zh_df[\"response_b\"] = response_bs\n",
    "zh_df[\"model_a\"] = model_as\n",
    "zh_df[\"model_b\"] = model_bs\n",
    "zh_df[\"language\"] = languages\n",
    "\n",
    "zh_df = zh_df[[\"id\", \"prompt\", \"response_a\", \"response_b\", \"model_a\", \"model_b\", \"language\"]]\n",
    "zh_df.to_parquet(f\"huggingface_api_Chinese-dpo-pairs_zh_{start_idx}_{end_idx}.parquet\", index=False)\n",
    "zh_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
